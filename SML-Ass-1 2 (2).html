<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html><head></head><body>



























































<div class="container-fluid main-container">




<div>



<h1 class="title toc-ignore">Statistical Machine Learning Ass. 1</h1>
<h4 class="author">Liam Heylin</h4>
<h4 class="date">2024-03-01</h4>

</div>


<p>Q1.</p>
<p>The aim is to use cross validation to evaluate 3 different models
using different classifiers:<br/>
C1 - all the shape and the material input variables.<br/>
C2 - only the shape input variables.<br/>
C3 - only the shape input variables.</p>
<p>On first inspection of the data, some of the first things to notice
are that it is not a very large sample size and there are very few
instances of the class ‘fail’ within the data relative to the other
classes ‘success’ and ‘near’. This is an example of an imbalanced
classification as the ‘fail’ class is far more rare than the others. In
the context of this data from an engineering perspective: predicting
True positives for ‘fail’ is more important than False positives as it
will lead to more caution even if it is overly cautious. Because there
is not a large number of observations within the data, the hold-out
method of cross validation is not appropriate. The method I will use is
K-fold cross validation which I will replicate R times. The accuracy of
each model and its class specific F1 score will be calculated and
compared for both in-sample and out-sample data.</p>
<pre class="r"><code>library(nnet)

load(&quot;data_assignment_1.RData&quot;)
set.seed(1991)

# set aside test data for testing
test &lt;- sample(1:nrow(data_exp), 10)
data_test &lt;- data_exp[test,]

# select data for training and validation
keep &lt;- setdiff(1:nrow(data_exp), test)
data &lt;- data_exp[keep,]


R &lt;- 150     # number of replications
K &lt;- 5       # number of folds
n &lt;- nrow(data)






#-------------
# C1-Model

#defining vectors for storing metrics
accuracy &lt;- vector(&quot;list&quot;, R)  
succ_F1 &lt;- vector(&quot;list&quot;, R)
near_F1 &lt;- vector(&quot;list&quot;, R)
fail_F1 &lt;- vector(&quot;list&quot;, R)

for ( r in 1:R ) {
  
  accuracy[[r]] &lt;- matrix(NA, K, 3)
  colnames(accuracy[[r]]) &lt;- c(&quot;fold&quot;, &quot;in-sample&quot;, &quot;out-sample&quot;)
  
  succ_F1[[r]] &lt;- matrix(NA, K, 3)
  colnames(succ_F1[[r]]) &lt;- c(&quot;fold&quot;, &quot;in-sample&quot;, &quot;out-sample&quot;)
  
  near_F1[[r]] &lt;- matrix(NA, K, 3)
  colnames(near_F1[[r]]) &lt;- c(&quot;fold&quot;, &quot;in-sample&quot;, &quot;out-sample&quot;)
  
  fail_F1[[r]] &lt;- matrix(NA, K, 3)
  colnames(fail_F1[[r]]) &lt;- c(&quot;fold&quot;, &quot;in-sample&quot;, &quot;out-sample&quot;)
  
  folds &lt;- rep( 1:K, ceiling(n/K) )
  folds &lt;- sample(folds)             # random permute 
  folds &lt;- folds[1:n] 
  # ensure we got n data points
  
  for ( k in 1:K ) {
    
    accuracy[[r]][k, &quot;fold&quot;] &lt;- k
    
    # split into folds
    train &lt;- which(folds != k)
    test &lt;- setdiff(1:n, train)
    
    # prevent information leakage
    data[train,-1] &lt;- scale(data[train,-1])
    data[test,-1] &lt;- scale(data[test,-1])
    
    # fit classifier on the training data (the folds except the k-th)
    lr &lt;- multinom(y ~ ., 
                   data = data[train,])
    
    # Classify the training data observations
    y_train &lt;- predict(lr, type = &quot;class&quot;, newdata = data[train, ])
    tab_train &lt;- table(truth = data[train, ]$y, y_train)
    
    # Calculate accuracy, class specific sensitivity, precision and F1 score
    # for train data
    c_sens &lt;- diag(tab_train)/rowSums(tab_train)
    c_prec &lt;- diag(tab_train)/colSums(tab_train)
    c_F1 &lt;- 2 * (c_sens * c_prec)/(c_sens + c_prec)
    
    accuracy[[r]][k, &quot;in-sample&quot;] &lt;- sum(diag(tab_train))/sum(tab_train)
    succ_F1[[r]][k, &quot;in-sample&quot;] &lt;- c_F1[&#39;success&#39;]
    near_F1[[r]][k, &quot;in-sample&quot;] &lt;- c_F1[&#39;near&#39;]
    fail_F1[[r]][k, &quot;in-sample&quot;] &lt;- c_F1[&#39;fail&#39;]
    
    # Classify the test data observations
    y_test &lt;- predict(lr, type = &quot;class&quot;, newdata = data[test, ])
    tab_test &lt;- table(truth = data[test, ]$y, y_test)
    
    # Calculate accuracy, class specific sensitivity, precision and F1 score
    # for validation data
    c_sens &lt;- diag(tab_test)/rowSums(tab_test)
    c_prec &lt;- diag(tab_test)/colSums(tab_test)
    c_F1 &lt;- 2 * (c_sens * c_prec)/(c_sens + c_prec)
    
    accuracy[[r]][k, &quot;out-sample&quot;] &lt;- sum(diag(tab_test))/sum(tab_test)
    succ_F1[[r]][k, &quot;out-sample&quot;] &lt;- c_F1[&#39;success&#39;]
    near_F1[[r]][k, &quot;out-sample&quot;] &lt;- c_F1[&#39;near&#39;]
    fail_F1[[r]][k, &quot;out-sample&quot;] &lt;- c_F1[&#39;fail&#39;]
    
    print( c(r, k) )
  }
} 



#Calculate and store the averages for each fold (in-sample and out-sample)
C1_acc_avg &lt;- t( sapply(accuracy, colMeans)[-1,] ) 
C1_succ_F1_avg &lt;- t( sapply(succ_F1, colMeans, na.rm = TRUE)[-1,])
C1_near_F1_avg &lt;- t( sapply(near_F1, colMeans, na.rm = TRUE)[-1,])
C1_fail_F1_avg &lt;- t(sapply(fail_F1, colMeans, na.rm = TRUE)[-1,])







#-------------
# C2 Model

#defining vectors for storing metrics
accuracy &lt;- vector(&quot;list&quot;, R)
succ_F1 &lt;- vector(&quot;list&quot;, R)
near_F1 &lt;- vector(&quot;list&quot;, R)
fail_F1 &lt;- vector(&quot;list&quot;, R)

for ( r in 1:R ) {
  
  accuracy[[r]] &lt;- matrix(NA, K, 3)
  colnames(accuracy[[r]]) &lt;- c(&quot;fold&quot;, &quot;in-sample&quot;, &quot;out-sample&quot;)
  
  succ_F1[[r]] &lt;- matrix(NA, K, 3)
  colnames(succ_F1[[r]]) &lt;- c(&quot;fold&quot;, &quot;in-sample&quot;, &quot;out-sample&quot;)
  
  near_F1[[r]] &lt;- matrix(NA, K, 3)
  colnames(near_F1[[r]]) &lt;- c(&quot;fold&quot;, &quot;in-sample&quot;, &quot;out-sample&quot;)
  
  fail_F1[[r]] &lt;- matrix(NA, K, 3)
  colnames(fail_F1[[r]]) &lt;- c(&quot;fold&quot;, &quot;in-sample&quot;, &quot;out-sample&quot;)
  
  folds &lt;- rep( 1:K, ceiling(n/K) )
  folds &lt;- sample(folds)             # random permute 
  folds &lt;- folds[1:n] 
  # ensure we got n data points
  
  for ( k in 1:K ) {
    
    accuracy[[r]][k, &quot;fold&quot;] &lt;- k
    
    # split into folds
    train &lt;- which(folds != k)
    test &lt;- setdiff(1:n, train)
    
    # prevent information leakage
    data[train,-1] &lt;- scale(data[train,-1])
    data[test,-1] &lt;- scale(data[test,-1])
    
    # fit classifier on the training data (the folds except the k-th)
    lr &lt;- multinom(y ~ shape_1 + shape_2 + shape_3, 
                   data = data[train,])
    
    # Classify the training data observations
    y_train &lt;- predict(lr, type = &quot;class&quot;, newdata = data[train, ])
    tab_train &lt;- table(truth = data[train, ]$y, y_train)
    
    # Calculate accuracy, class specific sensitivity, precision and F1 score
    # for train data
    c_sens &lt;- diag(tab_train)/rowSums(tab_train)
    c_prec &lt;- diag(tab_train)/colSums(tab_train)
    c_F1 &lt;- 2 * (c_sens * c_prec)/(c_sens + c_prec)
    
    accuracy[[r]][k, &quot;in-sample&quot;] &lt;- sum(diag(tab_train))/sum(tab_train)
    succ_F1[[r]][k, &quot;in-sample&quot;] &lt;- c_F1[&#39;success&#39;]
    near_F1[[r]][k, &quot;in-sample&quot;] &lt;- c_F1[&#39;near&#39;]
    fail_F1[[r]][k, &quot;in-sample&quot;] &lt;- c_F1[&#39;fail&#39;]
    
    # Classify the test data observations
    y_test &lt;- predict(lr, type = &quot;class&quot;, newdata = data[test, ])
    tab_test &lt;- table(truth = data[test, ]$y, y_test)
    
    # Calculate accuracy, class specific sensitivity, precision and F1 score
    # for validation data
    c_sens &lt;- diag(tab_test)/rowSums(tab_test)
    c_prec &lt;- diag(tab_test)/colSums(tab_test)
    c_F1 &lt;- 2 * (c_sens * c_prec)/(c_sens + c_prec)
    
    accuracy[[r]][k, &quot;out-sample&quot;] &lt;- sum(diag(tab_test))/sum(tab_test)
    succ_F1[[r]][k, &quot;out-sample&quot;] &lt;- c_F1[&#39;success&#39;]
    near_F1[[r]][k, &quot;out-sample&quot;] &lt;- c_F1[&#39;near&#39;]
    fail_F1[[r]][k, &quot;out-sample&quot;] &lt;- c_F1[&#39;fail&#39;]
    
    print( c(r, k) )
  }
} 


#Calculate and store the averages for each fold (in-sample and out-sample)
C2_acc_avg &lt;- t( sapply(accuracy, colMeans)[-1,] ) 
C2_succ_F1_avg &lt;- t( sapply(succ_F1, colMeans, na.rm = TRUE)[-1,])
C2_near_F1_avg &lt;- t( sapply(near_F1, colMeans, na.rm = TRUE)[-1,])
C2_fail_F1_avg &lt;- t(sapply(fail_F1, colMeans, na.rm = TRUE)[-1,])









#-------------
# C3 Model

#defining vectors for storing metrics
accuracy &lt;- vector(&quot;list&quot;, R)
succ_F1 &lt;- vector(&quot;list&quot;, R)
near_F1 &lt;- vector(&quot;list&quot;, R)
fail_F1 &lt;- vector(&quot;list&quot;, R)

for ( r in 1:R ) {
  
  accuracy[[r]] &lt;- matrix(NA, K, 3)
  colnames(accuracy[[r]]) &lt;- c(&quot;fold&quot;, &quot;in-sample&quot;, &quot;out-sample&quot;)
  
  succ_F1[[r]] &lt;- matrix(NA, K, 3)
  colnames(succ_F1[[r]]) &lt;- c(&quot;fold&quot;, &quot;in-sample&quot;, &quot;out-sample&quot;)
  
  near_F1[[r]] &lt;- matrix(NA, K, 3)
  colnames(near_F1[[r]]) &lt;- c(&quot;fold&quot;, &quot;in-sample&quot;, &quot;out-sample&quot;)
  
  fail_F1[[r]] &lt;- matrix(NA, K, 3)
  colnames(fail_F1[[r]]) &lt;- c(&quot;fold&quot;, &quot;in-sample&quot;, &quot;out-sample&quot;)
  
  folds &lt;- rep( 1:K, ceiling(n/K) )
  folds &lt;- sample(folds)             # random permute 
  folds &lt;- folds[1:n] 
  # ensure we got n data points
  
  for ( k in 1:K ) {
    
    accuracy[[r]][k, &quot;fold&quot;] &lt;- k
    
    # split into folds
    train &lt;- which(folds != k)
    test &lt;- setdiff(1:n, train)
    
    # prevent information leakage
    data[train,-1] &lt;- scale(data[train,-1])
    data[test,-1] &lt;- scale(data[test,-1])
    
    # fit classifier on the training data (the folds except the k-th)
    lr &lt;- multinom(y ~ material_1 + material_2 + material_3 + material_4, 
                   data = data[train,])
    
    # Classify the training data observations
    y_train &lt;- predict(lr, type = &quot;class&quot;, newdata = data[train, ])
    tab_train &lt;- table(truth = data[train, ]$y, y_train)
    
    
    # Calculate accuracy, class specific sensitivity, precision and F1 score
    # for train data
    c_sens &lt;- diag(tab_train)/rowSums(tab_train)
    c_prec &lt;- diag(tab_train)/colSums(tab_train)
    c_F1 &lt;- 2 * (c_sens * c_prec)/(c_sens + c_prec)
    
    accuracy[[r]][k, &quot;in-sample&quot;] &lt;- sum(diag(tab_train))/sum(tab_train)
    succ_F1[[r]][k, &quot;in-sample&quot;] &lt;- c_F1[&#39;success&#39;]
    near_F1[[r]][k, &quot;in-sample&quot;] &lt;- c_F1[&#39;near&#39;]
    fail_F1[[r]][k, &quot;in-sample&quot;] &lt;- c_F1[&#39;fail&#39;]
    
    # Classify the test data observations
    y_test &lt;- predict(lr, type = &quot;class&quot;, newdata = data[test, ])
    tab_test &lt;- table(truth = data[test, ]$y, y_test)
    
    # Calculate accuracy, class specific sensitivity, precision and F1 score
    # for validation data
    c_sens &lt;- diag(tab_test)/rowSums(tab_test)
    c_prec &lt;- diag(tab_test)/colSums(tab_test)
    c_F1 &lt;- 2 * (c_sens * c_prec)/(c_sens + c_prec)
    
    accuracy[[r]][k, &quot;out-sample&quot;] &lt;- sum(diag(tab_test))/sum(tab_test)
    succ_F1[[r]][k, &quot;out-sample&quot;] &lt;- c_F1[&#39;success&#39;]
    near_F1[[r]][k, &quot;out-sample&quot;] &lt;- c_F1[&#39;near&#39;]
    fail_F1[[r]][k, &quot;out-sample&quot;] &lt;- c_F1[&#39;fail&#39;]
    
    print( c(r, k) )
  }
} 


#Calculate and store the averages for each fold (in-sample and out-sample)
C3_acc_avg &lt;- t( sapply(accuracy, colMeans)[-1,] ) 
C3_succ_F1_avg &lt;- t( sapply(succ_F1, colMeans, na.rm = TRUE)[-1,])
C3_near_F1_avg &lt;- t( sapply(near_F1, colMeans, na.rm = TRUE)[-1,])
C3_fail_F1_avg &lt;- t(sapply(fail_F1, colMeans, na.rm = TRUE)[-1,])






# Tidy up results into a single dataframe
accuracy &lt;- data.frame(
  Accuracy = c(
    C1_acc_avg[,1], C1_acc_avg[,2],
    C2_acc_avg[,1], C2_acc_avg[,2],
    C3_acc_avg[,1], C3_acc_avg[,2]
  ),
  Classifier = rep(rep(c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;), each = R), 2),
  Sample_Type = rep(c(rep(&quot;in-sample&quot;, R), rep(&quot;out-sample&quot;, R)), 3)
)

set &lt;- factor(rep(1:2, each = R*3), labels = c(&quot;in-sample&quot;, &quot;out-sample&quot;))
cl &lt;- rep(factor(rep(1:3, each = R), labels = c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;)), 3)

# boxplot to show all results in a single panel
cols &lt;- c(&quot;darkorange3&quot;, &quot;deepskyblue3&quot;, &quot;magenta3&quot;)

boxplot(Accuracy ~ Classifier + Sample_Type, data = accuracy,
        border = cols, col = adjustcolor(cols, 0.1), xaxt = &quot;n&quot;,
        main = &#39;Accuracy&#39;)

mtext(side = 1, line = 1, at = c(2, 5, 8), text = levels(set))
legend(&quot;bottomright&quot;, legend = levels(cl), bty = &quot;n&quot;, fill = cols)</code></pre>
<p><img src="javascript://" width="960"/>
Looking at the boxplots of Accuracy for the three models, they all seem
to have similar size ranges and variability with few outliers when
evaluated for both the in-sample and out-sample data however the overall
scores for in-sample data are generally higher for each model than for
out-sample data. The C1 model is performing the best in both with
respect to this metric with a mean of around 0.81 for in-sample and 0.52
for out-sample and the C2 and C3 models are roughly the same as each
other.</p>
<pre class="r"><code># Tidy up results into a single dataframe
success_F1 &lt;- data.frame(
  Success_F1 = c(
    C1_succ_F1_avg[,1], C1_succ_F1_avg[,2],
    C2_succ_F1_avg[,1], C2_succ_F1_avg[,2],
    C3_succ_F1_avg[,1], C3_succ_F1_avg[,2]
  ),
  Classifier = rep(rep(c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;), each = R), 2),
  Sample_Type = rep(c(rep(&quot;in-sample&quot;, R), rep(&quot;out-sample&quot;, R)), 3)
)

# Tidy up results into a single dataframe
near_F1 &lt;- data.frame(
  Near_F1 = c(
    C1_near_F1_avg[,1], C1_near_F1_avg[,2],
    C2_near_F1_avg[,1], C2_near_F1_avg[,2],
    C3_near_F1_avg[,1], C3_near_F1_avg[,2]
  ),
  Classifier = rep(rep(c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;), each = R), 2),
  Sample_Type = rep(c(rep(&quot;in-sample&quot;, R), rep(&quot;out-sample&quot;, R)), 3)
)

# Tidy up results into a single dataframe
fail_F1 &lt;- data.frame(
  Fail_F1 = c(
    C1_fail_F1_avg[,1], C1_fail_F1_avg[,2],
    C2_fail_F1_avg[,1], C2_fail_F1_avg[,2],
    C3_fail_F1_avg[,1], C3_fail_F1_avg[,2]
  ),
  Classifier = rep(rep(c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;), each = R), 2),
  Sample_Type = rep(c(rep(&quot;in-sample&quot;, R), rep(&quot;out-sample&quot;, R)), 3)
)

set &lt;- factor(rep(1:2, each = R*3), labels = c(&quot;in-sample&quot;, &quot;out-sample&quot;))
cl &lt;- rep(factor(rep(1:3, each = R), labels = c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;)), 3)

# boxplot to show all results in a single panel
cols &lt;- c(&quot;darkorange3&quot;, &quot;deepskyblue3&quot;, &quot;magenta3&quot;)

par(mfrow = c(1, 3))

# Plot 1: Success F1
boxplot(Success_F1 ~ Classifier + Sample_Type, data = success_F1,
        border = cols, col = adjustcolor(cols, 0.1), xaxt = &quot;n&quot;,
        main = &#39;&quot;Success&quot; Specific F1 score&#39;)
mtext(side = 1, line = 1, at = c(2, 5), text = levels(set))
legend(&quot;bottomright&quot;, legend = levels(cl), bty = &quot;n&quot;, fill = cols)

# Plot 2: Near F1
boxplot(Near_F1 ~ Classifier + Sample_Type, data = near_F1,
        border = cols, col = adjustcolor(cols, 0.1), xaxt = &quot;n&quot;,
        main = &#39;&quot;Near&quot; Specific F1 score&#39;)
mtext(side = 1, line = 1, at = c(2, 5), text = levels(set))
legend(&quot;bottomright&quot;, legend = levels(cl), bty = &quot;n&quot;, fill = cols)

# Plot 3: Fail F1
boxplot(Fail_F1 ~ Classifier + Sample_Type, data = fail_F1,
        border = cols, col = adjustcolor(cols, 0.1), xaxt = &quot;n&quot;,
        main = &#39;&quot;Fail&quot; Specific F1 score&#39;)
mtext(side = 1, line = 1, at = c(2, 5), text = levels(set))
legend(&quot;bottomright&quot;, legend = levels(cl), bty = &quot;n&quot;, fill = cols)</code></pre>
<p><img src="javascript://" width="960"/>
The Class specific F1 scores give a better insight into the performance
of each model with respect to each class.</p>
<p>Starting from the left, for the class ‘success’, each of the models
performed better on the in-sample data than on the out-sample data with
a higher mean smaller range and variability. C1 again outperforms models
C2 and C3 with a mean of 0.84 for in-sample and 0.67 for out-sample.
Model C2 appears to outperform model C3 in in-sample and vice versa for
out-sample.</p>
<p>For the class ‘near’, there is slightly more variability in the
scores relative to the class ‘success’ and lower scores in general. The
model C1 still outperforms the other 2 models for both in-sample, with a
mean of 0.68, and out-sample, with a mean of 0.56.</p>
<p>Lastly for the class ‘fail’, the results are very different when
compared to the other two metrics. First of all, the F1 scores for
‘fail’ in the in-sample are far greater than in out-sample and the
variability is very large for out-sample. There are a lot more outliers
as well. I believe this can be accounted for by the lack of observations
of ‘fail’ within the data set. Because of this the model seems to
correctly predict all the true ’fail’s or not at all. This was the only
metric where, another model outperformed the C1 model on the out-sample
data. All of the models have a mean of 1 in the in-sample and then on
the out-sample model C2 performed the best but it has to be noted that
it has very large variability in its scores.</p>
<p>Based on this cross validation, the model C1 outperformed the models
C2 and C3 in almost every metric except for the ‘fail’ F1 specific score
on the out-sample data. It was also noted that in general, the models
performed better on the in-sample data than on the out-sample data with
higher means and smaller variability In conclusion, the best model in my
opinion is the C1 model.</p>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ul>
<p>Q2.</p>
<pre class="r"><code>C1 &lt;- multinom(y ~ ., data = data)


y_pred &lt;- predict(C1, type = &quot;class&quot;, newdata = data_test)
tab_test &lt;- table(truth = data_test$y, y_pred)


c_sens &lt;- diag(tab_test)/rowSums(tab_test)
c_prec &lt;- diag(tab_test)/colSums(tab_test)
c_F1 &lt;- 2 * (c_sens * c_prec)/(c_sens + c_prec)

accuracy &lt;- sum(diag(tab_test))/sum(tab_test)
c_F1 &lt;- replace(c_F1, is.nan(c_F1), 0)</code></pre>
<pre class="r"><code>print(tab_test)</code></pre>
<pre><code>##          y_pred
## truth     success near fail
##   success       0    0    4
##   near          0    0    3
##   fail          0    0    3</code></pre>
<pre class="r"><code>cat(&quot;Accuracy:\n&quot;, accuracy, &quot;\n\n success F1 Score:\n&quot;, c_F1[&#39;success&#39;], &quot;\n\n near F1 Score:\n&quot;, c_F1[&#39;near&#39;], &quot;\n\n fail F1 Score:\n&quot;, c_F1[&#39;fail&#39;])</code></pre>
<pre><code>## Accuracy:
##  0.3 
## 
##  success F1 Score:
##  0 
## 
##  near F1 Score:
##  0 
## 
##  fail F1 Score:
##  0.4615385</code></pre>
<p>Upon inspection of the metrics calculated on the test data, it’s
quite clear that the generalized predictive performance of the best
classifier at predicting the status of the component is pretty awful. An
accuracy of 0.4 is quite low in itself but the class specific F1 scores
are just appalling at 0, 0, and 0.4615385 for success, near and fail
respectively. Upon inspection of the truth table, its clear that the
model could not really differentiate between the classes with all of the
observations being classified as fail. The only positive I can see from
this output is that at least the model did not predict all of the
observations as success and also identified all the true positives for
fail. This is not reliable in the real world. I put this down to a lack
of initial data to train the models on as there was only 40 rows of data
and a severe lack of representation of the status fail. Just in case my
reasoning was incorrect, I also tested the models C2, and C3 on the test
data also and it turns out model C2 had slightly better results but
still very poor. I think had more data been provided, the models would
have performed far better.</p>




</div>















<script type="module" src="https://s.brightspace.com/lib/bsi/2025.10.239/unbundled/mathjax.js"></script><script type="text/javascript">document.addEventListener('DOMContentLoaded', function() {
						if (document.querySelector('math, .d2l-element, .d2l-cplus-layout') || /\$\$|\\\(|\\\[|\\begin{|\\ref{|\\eqref{/.test(document.body.innerHTML)) {
							document.querySelectorAll('mspace[linebreak="newline"]').forEach(elm => {
								elm.setAttribute('style', 'display: block; height: 0.5rem;');
							});

							document.querySelectorAll('math mmultiscripts > none').forEach(elm => {
								const mrow = document.createElementNS('http://www.w3.org/1998/Math/MathML', 'mrow');
								elm.replaceWith(mrow);
							});

							window.D2L.MathJax.loadMathJax({
								outputScale: 1.5,
								renderLatex: true,
								enableMML3Support: false
							});
						}
					});</script><script type="module" src="https://s.brightspace.com/lib/bsi/2025.10.239/unbundled/prism.js"></script><script type="text/javascript">document.addEventListener('DOMContentLoaded', function() {
					document.querySelectorAll('.d2l-code').forEach(code => {
						window.D2L.Prism.formatCodeElement(code);
					});
				});</script><script type="module" src="https://s.brightspace.com/lib/bsi/2025.10.239/unbundled/embeds.js"></script><script type="text/javascript">document.addEventListener('DOMContentLoaded', function() {
					window.D2L.EmbedRenderer.renderEmbeds(document.body);
				});</script></body></html>